{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装/导入 model lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "!pip install ../input/pywheels/loguru-0.5.3-py3-none-any.whl\n",
    "!pip install ../input/pywheels/einops-0.4.1-py3-none-any.whl\n",
    "!pip install ../input/pywheels/timm-0.4.12-py3-none-any.whl\n",
    "\n",
    "#kornia\n",
    "!pip install /kaggle/input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install /kaggle/input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n",
    "!pip install ../input/pywheels/pydegensac-0.1.2-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "\n",
    "!cp -r ../input/othermodels/github_QuadTreeAttention-master_renamed_pkg_src2 /kaggle/working/ \n",
    "!cd /kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/QuadTreeAttention/ && pip install .\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2\")\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/FeatureMatching/\")\n",
    "sys.path.append(\"/kaggle/working/github_QuadTreeAttention-master_renamed_pkg_src2/QuadTreeAttention/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "import PIL\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kornia\n",
    "from kornia_moons.feature import *\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "import random\n",
    "import string\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "src = '/kaggle/input/image-matching-challenge-2022/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 QTA\n",
    "from FeatureMatching.src2.config.default import get_cfg_defaults as get_cfg_defaults_qta\n",
    "from FeatureMatching.src2.utils.profiler import build_profiler as build_profiler_qta\n",
    "from FeatureMatching.src2.lightning.lightning_loftr import PL_LoFTR as PL_LoFTR_qta\n",
    "\n",
    "config = get_cfg_defaults_qta() # QTA config\n",
    "CROP_RATIO = 3 # QTA crop ratio\n",
    "TRANSFORMS = None # QTA transforms\n",
    "NB_EPOCHS = 5 # QTA epochs\n",
    "\n",
    "# INDOOT lofrt_ds_quadtree config\n",
    "config.LOFTR.MATCH_COARSE.MATCH_TYPE = 'dual_softmax' # QTA match type\n",
    "config.LOFTR.MATCH_COARSE.SPARSE_SPVS = False # QTA sparse spvs\n",
    "config.LOFTR.RESNETFPN.INITIAL_DIM = 128 # QTA initial dim\n",
    "config.LOFTR.RESNETFPN.BLOCK_DIMS=[128, 196, 256] # QTA block dims\n",
    "config.LOFTR.COARSE.D_MODEL = 256 # QTA model dim\n",
    "config.LOFTR.COARSE.BLOCK_TYPE = 'quadtree' # QTA block type\n",
    "config.LOFTR.COARSE.ATTN_TYPE = 'B' # QTA attention type\n",
    "config.LOFTR.COARSE.TOPKS=[32, 16, 16] # QTA topks\n",
    "config.LOFTR.FINE.D_MODEL = 128 # QTA model dim\n",
    "config.TRAINER.WORLD_SIZE = 1 # QTA world size\n",
    "config.TRAINER.CANONICAL_BS = 32 # QTA canonical batch size\n",
    "config.TRAINER.TRUE_BATCH_SIZE = 1 # QTA true batch size\n",
    "_scaling = 1 # QTA scaling\n",
    "config.TRAINER.ENABLE_PLOTTING = False # QTA plotting\n",
    "config.TRAINER.SCALING = _scaling # QTA scaling\n",
    "config.TRAINER.TRUE_LR = 1e-3 # 1e-4 config.TRAINER.CANONICAL_LR * _scaling\n",
    "config.TRAINER.WARMUP_STEP = 0 #math.floor(config.TRAINER.WARMUP_STEP / _scaling)\n",
    "\n",
    "# lightning module\n",
    "qta_max_img_size = 1024 # max image size for QTA\n",
    "\n",
    "qta_device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device\n",
    "disable_ckpt = True # disable checkpoint\n",
    "profiler_name = None # help='options: [inference, pytorch], or leave it unset\n",
    "qta_profiler = build_profiler_qta(profiler_name) # QTA profiler\n",
    "qta_model = PL_LoFTR_qta(config, # QTA model\n",
    "                 pretrained_ckpt= \"/kaggle/working/outdoor_quadtree.ckpt\", # args.ckpt_path, from scratch atm\n",
    "                 profiler=qta_profiler # QTA profiler\n",
    "                 )\n",
    "qta_matcher = qta_model.matcher # get matcher\n",
    "qta_matcher.eval() # set eval mode\n",
    "qta_matcher.to(qta_device) # move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image utils\n",
    "def load_resized_image(fname, max_image_size):\n",
    "    '''\n",
    "    扩大至指定尺寸\n",
    "    '''\n",
    "    img = cv2.imread(fname) # load image\n",
    "    scale = max_image_size / max(img.shape[0], img.shape[1])  # get scale\n",
    "    w = int(img.shape[1] * scale) # weight\n",
    "    h = int(img.shape[0] * scale) # height\n",
    "    img = cv2.resize(img, (w, h)) \n",
    "    return img, scale\n",
    "\n",
    "def scale_to_resized(mkpts0, mkpts1, scale1, scale2):\n",
    "    \n",
    "    # first point\n",
    "    mkpts0[:, 0] = mkpts0[:, 0] / scale1\n",
    "    mkpts0[:, 1] = mkpts0[:, 1] / scale1\n",
    "    \n",
    "    # second point\n",
    "    mkpts1[:, 0] = mkpts1[:, 0] / scale2\n",
    "    mkpts1[:, 1] = mkpts1[:, 1] / scale2\n",
    "    \n",
    "    return mkpts0, mkpts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QTA Utils\n",
    "def load_loftr_image_orig(fname):\n",
    "    img0_raw = cv2.imread(fname, cv2.IMREAD_GRAYSCALE) \n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255. \n",
    "    return img0\n",
    "\n",
    "def put_img_on_disk(img, output_img_tag):\n",
    "    img_path_on_disk = f'/kaggle/working/{output_img_tag}.png'\n",
    "    cv2.imwrite(img_path_on_disk, img) \n",
    "    return img_path_on_disk\n",
    "\n",
    "def calc_divide_size_smallest(im_size, coef):\n",
    "    \n",
    "    \n",
    "    if im_size % coef == 0:\n",
    "        return im_size  \n",
    "    return round(((im_size / coef) + 0.5)) * coef\n",
    "\n",
    "\n",
    "def add_zero_padding_two_img_same(img1, img2, div_coef=32):\n",
    "    '''\n",
    "    add zero padding to two images to make them same size\n",
    "    '''\n",
    "\n",
    "    img1_height, img1_width, img1_channels = img1.shape \n",
    "    img2_height, img2_width, img2_channels = img2.shape \n",
    "    \n",
    "    \n",
    "    max_width = max(img1_width, img2_width) \n",
    "    max_height = max(img1_height, img2_height) \n",
    "    \n",
    "    \n",
    "    result1, offset1 = create_zero_padding_img(img1, max_width, max_height, img1_channels, div_coef) \n",
    "    result2, offset2 = create_zero_padding_img(img2, max_width, max_height, img2_channels, div_coef) \n",
    "    \n",
    "    return result1, result2, offset1, offset2\n",
    "\n",
    "\n",
    "def create_zero_padding_img(img, max_im_width, max_im_height, channels, div_coef=32):\n",
    "    \n",
    "    new_area_image_width = calc_divide_size_smallest(max_im_width, div_coef) \n",
    "    new_area_image_height = calc_divide_size_smallest(max_im_height, div_coef) \n",
    "    \n",
    "    \n",
    "    im_height, im_width, im_channels = img.shape \n",
    "    x_offset = (new_area_image_width - im_width) // 2 \n",
    "    y_offset = (new_area_image_height - im_height) // 2 \n",
    "    \n",
    "    im_right  = x_offset + im_width  \n",
    "    im_bottom = y_offset + im_height \n",
    "    \n",
    "    \n",
    "    color = (0,0,0) \n",
    "    result = np.full((new_area_image_height, new_area_image_width, im_channels), color, dtype=np.uint8) \n",
    "    \n",
    "    \n",
    "    result[y_offset:im_bottom, x_offset:im_right] = img \n",
    "    \n",
    "    \n",
    "    return result, (x_offset, y_offset) \n",
    "\n",
    "\n",
    "def unpad_matches(mkpts0, mkpts1, offset_point1, offset_point2):\n",
    "    '''\n",
    "    remove padding from two images\n",
    "    '''\n",
    "    offset_x1, offset_y1 = offset_point1 \n",
    "    offset_x2, offset_y2 = offset_point2 \n",
    "    \n",
    "     # 去除偏移量\n",
    "    mkpts0[:, 0] = mkpts0[:, 0] - offset_x1\n",
    "    mkpts0[:, 1] = mkpts0[:, 1] - offset_y1\n",
    "    \n",
    "    mkpts1[:, 0] = mkpts1[:, 0] - offset_x2\n",
    "    mkpts1[:, 1] = mkpts1[:, 1] - offset_y2\n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "\n",
    "\n",
    "# QTA inference\n",
    "def qta_inference(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32):\n",
    "    \n",
    "    img1_resized, scale1 = load_resized_image(image_fpath_1, max_image_size) \n",
    "    img2_resized, scale2 = load_resized_image(image_fpath_2, max_image_size) \n",
    "    \n",
    "    \n",
    "    pad_img1, pad_img2, pad_offset_p1, pad_offset_p2 = add_zero_padding_two_img_same(img1_resized, img2_resized, divide_coef) \n",
    "\n",
    "     \n",
    "    img1_disk_path = put_img_on_disk(pad_img1, 'qta_img1')\n",
    "    img2_disk_path = put_img_on_disk(pad_img2, 'qta_img2')\n",
    "    \n",
    "   \n",
    "    gray_img_1 = load_loftr_image_orig(img1_disk_path) \n",
    "    gray_img_2 = load_loftr_image_orig(img2_disk_path)\n",
    "    \n",
    "    batch = {'image0': gray_img_1, 'image1': gray_img_2}\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        qta_matcher.eval() \n",
    "        qta_matcher.to(qta_device) \n",
    "        \n",
    "        qta_matcher(batch) \n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy() \n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy() \n",
    "    \n",
    "    \n",
    "    mkpts0, mkpts1 = unpad_matches(mkpts0, mkpts1, pad_offset_p1, pad_offset_p2) \n",
    "    \n",
    "    \n",
    "    mkpts0, mkpts1 = scale_to_resized(mkpts0, mkpts1, scale1, scale2) \n",
    "    \n",
    "    \n",
    "    if os.path.exists(img1_disk_path): os.remove(img1_disk_path)\n",
    "    if os.path.exists(img2_disk_path): os.remove(img2_disk_path) \n",
    "    \n",
    "    return mkpts0, mkpts1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoFTR + SuperGlue + DKM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('../input/othermodels/github_LoFTR-master') \n",
    "\n",
    "\n",
    "\n",
    "kornia_max_image_size = 1120 # max image size for LoFTR\n",
    "kornia_at_least_matches = 280 # at least matches for LoFTR\n",
    "kornia_thrs_conf_match = 0.3 # threshold for confidence match for LoFTR\n",
    "kornia_max_matches = 1000 # -1 -> use all\n",
    "\n",
    "kf_loftr_out_device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda') # use cpu or cuda\n",
    "kf_loftr_out_matcher = KF.LoFTR(pretrained=None) # load model\n",
    "kf_loftr_out_matcher.load_state_dict(torch.load(\"/kaggle/input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict']) # load weights\n",
    "kf_loftr_out_matcher = kf_loftr_out_matcher.to(kf_loftr_out_device).eval() # set to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SuperGlue\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.utils import (AverageTimer, read_image)\n",
    "\n",
    "sg_device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "resize = [1600, ] \n",
    "resize_float = True \n",
    "\n",
    "\n",
    "config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.001, \n",
    "        \"max_keypoints\": 1280 \n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\", \n",
    "        \"sinkhorn_iterations\": 20, \n",
    "        \"match_threshold\": 0.1,\n",
    "    }\n",
    "}\n",
    "sg_matching = Matching(config).eval().to(sg_device) \n",
    "\n",
    "\n",
    "use_sg_filter_strategy = False \n",
    "sg_max_confid_matches = 300 \n",
    "sg_thrs_conf_match = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DKM\n",
    "!mkdir -p pretrained/checkpoints\n",
    "!cp ../input/othermodels/dkm_base_v11.pth pretrained/checkpoints/dkm_base_v11.pth\n",
    "\n",
    "!pip install -f ../input/pywheels --no-index einops \n",
    "!cp -r ../input/othermodels/github_dkm-main_fix_kaggle/ /kaggle/working/DKM/\n",
    "\n",
    "sys.path.append('/kaggle/working/DKM/')\n",
    "\n",
    "# DKM\n",
    "import torch\n",
    "torch.hub.set_dir('/kaggle/working/pretrained/')\n",
    "from dkm import dkm_base\n",
    "\n",
    " # load model\n",
    "dkm_model = dkm_base(\n",
    "                pretrained=True,\n",
    "                version=\"v11\",\n",
    "                use_cuda=True, \n",
    "                request_im_size=(672, 896) \n",
    "                     )\n",
    "\n",
    "dkm_ft_num = 1000       \n",
    "dkm_max_confid_matches = 200 \n",
    "dkm_thrs_conf_match = 0.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device, max_image_size):\n",
    "    '''\n",
    "    扩大至指定尺寸\n",
    "    '''\n",
    "    img = cv2.imread(fname)\n",
    "    scale = max_image_size / max(img.shape[0], img.shape[1])  \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img.to(device), scale\n",
    "\n",
    "\n",
    "def filter_conf_matches(mkpts0, mkpts1, mconf, thrs_conf_match=0.2, max_matches=-1):\n",
    "  \n",
    "    # sort points by confidence descending\n",
    "    mkps1_sorted = [x for (y,x) in sorted(zip(mconf,mkpts0), key=lambda pair: pair[0], reverse=True)] \n",
    "    mkps2_sorted = [x for (y,x) in sorted(zip(mconf,mkpts1), key=lambda pair: pair[0], reverse=True)] \n",
    "    \n",
    "    # overwrite\n",
    "    mkps1 = np.array(mkps1_sorted) \n",
    "    mkps2 = np.array(mkps2_sorted)\n",
    "\n",
    "    num_thrsh_greater = (mconf >= thrs_conf_match).sum() \n",
    "\n",
    "    take_first_el = min(max_matches, num_thrsh_greater) \n",
    "    if take_first_el > 0 and len(mkps1) > take_first_el:\n",
    "        mkps1 = mkps1[:take_first_el] \n",
    "        mkps2 = mkps2[:take_first_el] \n",
    "    return mkps1, mkps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loftr_inference(fname1, fname2, max_image_size):    \n",
    "    '''\n",
    "    Infer matches using LoFTR.\n",
    "    '''\n",
    "    image_1, scale1 = load_torch_image(fname1, kf_loftr_out_device, max_image_size) \n",
    "    image_2, scale2 = load_torch_image(fname2, kf_loftr_out_device, max_image_size)\n",
    "    \n",
    "   \n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1),\n",
    "                  \"image1\": K.color.rgb_to_grayscale(image_2)\n",
    "              }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correspondences = kf_loftr_out_matcher(input_dict) \n",
    "        \n",
    "   \n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy() \n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy() \n",
    "    \n",
    "   \n",
    "    mkpts0 = mkpts0 / scale1 \n",
    "    mkpts1 = mkpts1 / scale2     \n",
    "    \n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "\n",
    "def superglue_inference(image_fpath_1, image_fpath_2):    \n",
    "    image_1, inp_1, scales_1 = read_image(image_fpath_1, sg_device, resize, 0, resize_float) \n",
    "    image_2, inp_2, scales_2 = read_image(image_fpath_2, sg_device, resize, 0, resize_float) \n",
    "    pred = sg_matching({\"image0\": inp_1, \"image1\": inp_2}) \n",
    "    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()} \n",
    "    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"] \n",
    "    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"] \n",
    "\n",
    "  \n",
    "    valid = matches > -1 \n",
    "    mkpts1 = kpts1[valid]  \n",
    "    mkpts2 = kpts2[matches[valid]] \n",
    "    mconf = conf[valid] \n",
    "    \n",
    "    \n",
    "    scale1, scale2 = scales_1 \n",
    "    \n",
    "\n",
    "    mkpts1, mkpts2 = scale_to_resized(mkpts1, mkpts2, 1./scale1, 1./scale2)\n",
    "        \n",
    "    # filter matches by confidence\n",
    "    if use_sg_filter_strategy:\n",
    "        mkpts1, mkpts2 = filter_conf_matches(   \n",
    "            mkpts1,\n",
    "            mkpts2,\n",
    "            mconf, \n",
    "            thrs_conf_match=sg_thrs_conf_match, \n",
    "            max_matches=sg_max_confid_matches,  \n",
    "            )\n",
    "    \n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "\n",
    "def dkm_inference(image_fpath_1, image_fpath_2):\n",
    "   \n",
    "    img1 = cv2.imread(image_fpath_1) \n",
    "    img2 = cv2.imread(image_fpath_2) \n",
    "\n",
    "    \n",
    "    img1PIL = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)) \n",
    "    img2PIL = Image.fromarray(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)) \n",
    "    \n",
    "    \n",
    "    dense_matches, dense_certainty = dkm_model.match(img1PIL, img2PIL, use_cuda=True) \n",
    "    \n",
    "    \n",
    "    dense_certainty = dense_certainty.sqrt() \n",
    "    \n",
    "    sparse_matches, sparse_certainty = dkm_model.sample(dense_matches, dense_certainty, dkm_ft_num) \n",
    "\n",
    "    mkps1 = sparse_matches[:, :2] \n",
    "    mkps2 = sparse_matches[:, 2:] \n",
    "\n",
    "    \n",
    "    h, w, c = img1.shape \n",
    "    mkps1[:, 0] = ((mkps1[:, 0] + 1)/2) * w\n",
    "    mkps1[:, 1] = ((mkps1[:, 1] + 1)/2) * h\n",
    "\n",
    "    h, w, c = img2.shape\n",
    "    mkps2[:, 0] = ((mkps2[:, 0] + 1)/2) * w\n",
    "    mkps2[:, 1] = ((mkps2[:, 1] + 1)/2) * h\n",
    "    \n",
    "    \n",
    "    \n",
    "    mkps1, mkps2 = filter_conf_matches(\n",
    "        mkps1,  \n",
    "        mkps2,  \n",
    "        sparse_certainty,  \n",
    "        thrs_conf_match=dkm_thrs_conf_match, \n",
    "        max_matches=dkm_max_confid_matches,  \n",
    "        )\n",
    "    \n",
    "    return mkps1, mkps2, sparse_certainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_models_ensemble(image_fpath_1, image_fpath_2):\n",
    "    '''\n",
    "    Input : image1 和 image2, 进行inference ensemble\n",
    "    Return: match keypoints \n",
    "    '''\n",
    "    # loftr\n",
    "    k_mkpts1, k_mkpts2 = loftr_inference(image_fpath_1, image_fpath_2, kornia_max_image_size) \n",
    "    mkpts1_merge = k_mkpts1\n",
    "    mkpts2_merge = k_mkpts2\n",
    "    \n",
    "    #  QuadTreeAttention\n",
    "    qta_mkps1,qta_mkps2 = qta_inference(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32) \n",
    "    mkpts1_merge = np.concatenate((mkpts1_merge, qta_mkps1), axis=0)\n",
    "    mkpts2_merge = np.concatenate((mkpts2_merge, qta_mkps2), axis=0)\n",
    "    \n",
    "    # SUPERGLUE\n",
    "    sg_mkpts1, sg_mkpts2 = superglue_inference(image_fpath_1, image_fpath_2) \n",
    "    mkpts1_merge = np.concatenate((mkpts1_merge, sg_mkpts1), axis=0)\n",
    "    mkpts2_merge = np.concatenate((mkpts2_merge, sg_mkpts2), axis=0)\n",
    "\n",
    "    # DKM\n",
    "    dkm_mkpts1, dkm_mkpts2, _ = dkm_inference(image_fpath_1, image_fpath_2)\n",
    "    if len(dkm_mkpts1) > 0:\n",
    "        mkpts1_merge = np.concatenate((mkpts1_merge, dkm_mkpts1), axis=0)\n",
    "        mkpts2_merge = np.concatenate((mkpts2_merge, dkm_mkpts2), axis=0)\n",
    "\n",
    "    return mkpts1_merge, mkpts2_merge # get match keypoints\n",
    "\n",
    "\n",
    "def calc_F_matrix(item):\n",
    "    '''\n",
    "    Input : sample_id, mkps1, mkps2 # 一对匹配关键点\n",
    "    Return: F-matrix\n",
    "    '''\n",
    "    sample_id, mkps1, mkps2 = item \n",
    "\n",
    "    if len(mkps1) > 7: \n",
    "        F, _ = cv2.findFundamentalMat(mkps1, mkps2, cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.999999, maxIters=120_000)  # 计算F-matrix\n",
    "    else:   \n",
    "        F = None   \n",
    "\n",
    "    del mkps1, mkps2 # clean up\n",
    "    return sample_id, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n",
    "        \n",
    "test_samples_run_optim = test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "from concurrent import futures\n",
    "F_dict = {} # F-matrix\n",
    "prev_matches_item = None # previous matches item\n",
    "\n",
    "for i, (sample_id, batch_id, image_1_id, image_2_id) in enumerate(test_samples_run_optim):\n",
    "    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png' \n",
    "    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png' \n",
    "    \n",
    "    if prev_matches_item is None:\n",
    "        \n",
    "        mkpts1_merge, mkpts2_merge = inf_models_ensemble(image_fpath_1, image_fpath_2) \n",
    "        prev_matches_item = (sample_id, mkpts1_merge, mkpts2_merge) \n",
    "    else:\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            future1 = executor.submit(inf_models_ensemble, image_fpath_1=image_fpath_1, image_fpath_2=image_fpath_2) \n",
    "            future2 = executor.submit(calc_F_matrix, item=prev_matches_item) \n",
    "            future_list = [future1,future2]\n",
    "\n",
    "            finished, pending = futures.wait(\n",
    "                future_list, \n",
    "                return_when=futures.ALL_COMPLETED \n",
    "                )\n",
    "            \n",
    "            mkpts1_merge, mkpts2_merge = future1.result()\n",
    "            old_sample_id, F = future2.result()\n",
    "\n",
    "            # results are ready\n",
    "            F_dict[old_sample_id] = np.zeros((3, 3)) if F is None else F  \n",
    "            prev_matches_item = (sample_id, mkpts1_merge, mkpts2_merge) \n",
    "    \n",
    "    try:\n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "if prev_matches_item is not None:\n",
    "    sample_id, F = calc_F_matrix(prev_matches_item) \n",
    "    F_dict[sample_id] = np.zeros((3, 3)) if F is None else F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FlattenMatrix(M, num_digits=8):\n",
    "    \n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "# 写入 \n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('sample_id,fundamental_matrix\\n')\n",
    "    for sample_id, F in F_dict.items():\n",
    "        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
